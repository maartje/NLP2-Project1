%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}
\usepackage{graphicx}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Machine Translation Models IBM 1 and IBM 2}

\author{First Author \\
  Maartje de Jonge \\
  0194107 \\
  {\tt maartjedejonge@gmail.com} \\\And
  Second Author \\
  Lina Murady \\
  xxx \\
  {\tt lina.murady@gmail.com} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}

Scope: empirical evaluation of IBM 1 and IBM 2 models 

Contributions: 
- performance of the different models on test data 
- analyses of the models and their performance

\end{abstract}

\section{Introduction}

% Background: translation models
- Statistical Machine Translation
- Baysian split: $p(e|f) \propto p(e)*p(f|e)$ 
- We focus on translation model $p(f|e)$

% Background: alignment models
- alignment model: word pairs $(f,e)$ with the constraint that each french word 
matches exactly one english word. 
- Null word added to english sentence to align words in french that do
not have an equivalent word in english (insertions)

% Background: IBM 1
- decomposition into: sentence length probability, alignment prob,
translation prob
(- alignment prob mixture component)
- IBM 1: assume uniform alignment probability, 
- train with EM (explain why) 

% Background: IBM 2
- Shortcomings of IBM 1 (assumption uniform alignments)
- IBM 2 learn probabilities  $p(i,j,I,J)$
- problem: too many parameters for small training sets
- approach: jump probabilities \citep{Vogel}, 
model probabilities as jumps from diagonal.
- train with EM

% Background: IBM 1 Variational Inference
- Problem with maximum likelihood estimaion, arguments for Bayesian approach
- Problem with posterior inference which motivates variational inference
- We use Dirichlet Prior and Variational Inference to meet these
limitations

% Focus, Problem: comparison of IBM models
- In this report we compare alignment models IBM 1, IBM 2
and IB 1 with variational inference
- We empirically evaluate how these models perform on a 
corpus and we discuss their differences


% outline
- Section \ref{ExperimentalSetup}
- Section \ref{IBM1}
- Section \ref{IBM1_Dirichlet}
- Section \ref{IBM2}
 
\section{Models}

\subsection{IBM 1}
- describe the model mathematically
- mathematical assumptions
- factorisation
- parameterisation
- limitations
- parameter estimation: EM
- inference techniques: viterbi alignment

- cite some literature

\subsection{IBM 2}
- describe the model mathematically
- mathematical assumptions
- factorisation
- parameterisation
- limitations
- parameter estimation: EM
- inference techniques: viterbi alignment

- cite some literature

\subsection{IBM 1 with Variational Inference}

- describe the model mathematically
- mathematical assumptions
- factorisation
- parameterisation
- limitations
- parameter estimation: variational inference
- inference techniques: viterbi alignment

- cite some literature

\paragraph{Jump Parameterization}
- why: lot of parameters for small data set
- math: formula
- intuition: diagonal
- literature: vogel


\section{Experiments}
\label{Experiments}

\subsection{Experimental Setup}
\label{ExperimentalSetup}

- datasets: training, validation, test
- numbers, languages, where does the data come from?

- setup

- Viterbi Alignment: how do we deal with unknown words in the validation/test set

- metric: AER

- stop/convergence criteria:
1) based on training log likelihood
Relative log-likelihood convergence: 
$\frac{ll_i - ll_{i-1}}{ll_{i-1}} < \epsilon$
why relative? what epsilon?

2) best AER on validation set
Absolute criterion. why? what epsilon?
$prevAER - AER < 0$

\subsection{IBM 1 with Expectation Maximization}
\label{IBM1}


\paragraph{Training Conditions}

- uniform initialisation

\paragraph{Results}

\input{fig-ibm1-iterations.tex}

- Figure: training log-likelihood vs iteration

- Figure: validation AER vs iteration

- Figure/tabel: AER on test set using model selected based on AER and based on log-likelihood [remark: use official tool instead of python code]  


\subsection{IBM 1 with Variational Inference}
\label{IBM1_Dirichlet}


\paragraph{Training Conditions}

- uniform initialization

- choice of hyper parameter

\paragraph{Results}

- Figure: training log-likelihood vs iteration

- Figure: validation AER vs iteration

- Figure/tabel: AER on test set using model selected based on AER and based on log-likelihood [remark: use official tool instead of python code]  


\subsection{IBM 2 with Expectation Maximization}
\label{IBM2}

\paragraph{Training Conditions}

- initialization
Non-convex, thus local minimum, result depends on initialization
1) uniform 
2) random 3 times 
3) staged, use result of model 1 run


\paragraph{Results}

- Figure: training log-likelihood vs iteration
using different initializations
a) uniform 
b) random 3 times 
c) staged 

- Figure: validation AER vs iteration
using different initializations
a) uniform 
b) random 3 times 
c) staged 

- Figure/tabel: AER on test set using model selected based on AER and based on log-likelihood [remark: use official tool instead of python code]  
compare IBM 1 with IBM 2

\subsection{Discussion}

- (non)-convexity 
- stability 
- convergence.

- complexity
- qualitative insight: 
i.e. distributions for rare words, frequent words and jump distribution


\section{Conclusion and Future Work}

- Future work: Dirichlet on IBM 2

- Future work: IBM 2 with jumps for other languages with completely different word order,
i.e. not most probability mass on diagonal

- Comparison: Which model is the best, why?

- contributions 

- limitations

\end{document}
