{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import datasets\n",
    "import debug_helpers\n",
    "import datetime\n",
    "\n",
    "import collections\n",
    "import aer\n",
    "import warnings\n",
    "\n",
    "# pretty print variabeles on line\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns list with jump probabilities \n",
    "# (and special NULL jump probability)\n",
    "# [-max_jump ....0 ...max_jump, NULL_jump]\n",
    "def _initialize_jump_probs(sentence_pairs):\n",
    "    max_jump = 0\n",
    "    for (s_sentence, t_sentence) in sentence_pairs:\n",
    "        s_length = len(s_sentence) - 1 #ignore NULL\n",
    "        t_length = len(t_sentence)\n",
    "        jump_1 = abs(1 - math.floor(s_length))\n",
    "        jump_2 = abs(s_length - math.floor(s_length/t_length))\n",
    "        max_jump = max(jump_1, jump_2, max_jump)\n",
    "    init_prob = 1. / (2*max_jump + 1 + 1) # last item is special NULL jump prob\n",
    "    return [init_prob] * (2*max_jump + 1 + 1)\n",
    "\n",
    "# returns the index in the jump_probabilities list\n",
    "# for given source and target positions and sentence lengths\n",
    "# s_pos and s_length for source sentence including the special NULL word\n",
    "# sentence positions start at index 0\n",
    "def get_jump_prob_index(s_pos, t_pos, s_length, t_length, jump_probs):\n",
    "    if s_pos == 0:\n",
    "        return len(jump_probs) - 1\n",
    "    jump = int(s_pos - math.floor((t_pos + 1) * (s_length - 1) / t_length))\n",
    "    max_jump = int((len(jump_probs) - 2)/2)\n",
    "    jump_prob_index = jump + max_jump\n",
    "    if jump_prob_index < 0:\n",
    "        warnings.warn(\n",
    "            f'Jump prob index {jump_prob_index} (jump:{jump}) out of range.'\n",
    "        )\n",
    "        return 0 #approximate with prob of largest negative jump\n",
    "    if jump_prob_index >= len(jump_probs) - 1:\n",
    "        warnings.warn(\n",
    "            f'Jump prob index {jump_prob_index} (jump:{jump}) out of range.'\n",
    "        )\n",
    "        return len(jump_probs) - 2 #approximate with prob of largest positive jump\n",
    "\n",
    "    return jump_prob_index\n",
    "\n",
    "# returns jump probability for given source and target positions\n",
    "# and lengths. Positions start at index 0, source sentence contains\n",
    "# special NULL word at position 0\n",
    "def get_jump_prob(s_pos, t_pos, s_length, t_length, jump_probs):\n",
    "    jump_prob_index = get_jump_prob_index(s_pos, t_pos, s_length, t_length, jump_probs)\n",
    "    return jump_probs[jump_prob_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# {Hause: { book:0.25, ...}, ...}\n",
    "# read: the probability of 'book' given 'Haus' is 0.25\n",
    "def _initialize_lexicon_probabilities(source_vocabulary, target_vocabulary):\n",
    "    p_init = 1./len(target_vocabulary)\n",
    "    lexicon_probabilities = collections.defaultdict(\n",
    "        lambda: collections.defaultdict(lambda: p_init))\n",
    "    return lexicon_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def source_dependencies(s_sentence, t_word, t_pos, t_length, lprobs, jump_probs):\n",
    "    s_length = len(s_sentence)    \n",
    "    jump_probs = [get_jump_prob(\n",
    "        s_word_pos, t_pos, s_length, t_length, jump_probs\n",
    "    ) for s_word_pos in range(s_length)]\n",
    "    sum_jump_probs = sum(jump_probs)\n",
    "    \n",
    "    return [\n",
    "        (jump_probs[s_pos]/ sum_jump_probs) * lprobs[s_word][t_word] \n",
    "        for s_pos, s_word in enumerate(s_sentence)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align(lprobs, jump_probs, sentence_pairs):\n",
    "    if isinstance(sentence_pairs, tuple):\n",
    "        return _align_sentence_pair(lprobs, jump_probs, sentence_pairs)\n",
    "    return [ _align_sentence_pair(lprobs, jump_probs, sentence_pair) for sentence_pair in sentence_pairs ]\n",
    "\n",
    "def _align_sentence_pair(lprobs, jump_probs, sentence_pair):\n",
    "    s_sentence = sentence_pair[0]\n",
    "    t_sentence = sentence_pair[1]\n",
    "    s_length = len(s_sentence)\n",
    "    t_length = len(t_sentence)\n",
    "    best_alignment = set()\n",
    "    for t_pos, t_word in enumerate(t_sentence):\n",
    "        # TODO: prevent double loop\n",
    "        source_dependencies = source_dependencies(s_sentence, t_word, t_pos, t_length, lprobs, jump_probs)\n",
    "        (best_align_pos, _) = max(enumerate(source_dependencies), key=lambda t: t[1])\n",
    "        if (best_align_pos > 0): # Leave out NULL-alignments (and alignments between unseen words)\n",
    "            best_alignment.add((best_align_pos, t_pos + 1)) # word positions start at 1\n",
    "    return best_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM(s_t_pairs, s_vocabulary, t_vocabulary, max_iterations = 10,\n",
    "        val_sentence_pairs = None, reference_alignments = None, fn_debug = None):\n",
    "    lprobs = _initialize_lexicon_probabilities(s_vocabulary, t_vocabulary)\n",
    "    jump_probs = _initialize_jump_probs(s_t_pairs)\n",
    "#    print('initial jump probs', jump_probs)\n",
    "    i = 0\n",
    "    log_likelihoods = []\n",
    "    AERs = []\n",
    "    while i < max_iterations:\n",
    "        print(datetime.datetime.now().strftime(\"%I:%M\"), ':', i)\n",
    "        \n",
    "        # initialize\n",
    "        log_likelihood = 0\n",
    "        AER = 0\n",
    "        counts_t_given_s = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "        total_s = collections.defaultdict(int)\n",
    "        jump_counts = [0]*len(jump_probs)\n",
    "\n",
    "        # calculate counts and log likelihood\n",
    "        for (s_sentence, t_sentence) in s_t_pairs:\n",
    "            s_length = len(s_sentence)\n",
    "            t_length = len(t_sentence)\n",
    "            for t_pos, t_word in enumerate(t_sentence):\n",
    "                prob_counts = source_dependencies(\n",
    "                    s_sentence, t_word, t_pos, t_length, lprobs, jump_probs)\n",
    "                s_total_t = sum(prob_counts)\n",
    "                log_likelihood += math.log(s_total_t)\n",
    "                                \n",
    "                for s_pos, s_word in enumerate(s_sentence):\n",
    "                    update = prob_counts[s_pos]/s_total_t\n",
    "                    counts_t_given_s[s_word][t_word] += update\n",
    "                    total_s[s_word] += update\n",
    "                    jump_count_index = get_jump_prob_index(s_pos, t_pos, s_length, t_length, jump_probs)\n",
    "                    jump_counts[jump_count_index] += update \n",
    " #                   print('jump_update', jump_count_index, '%.2f' % update, s_word, t_word)\n",
    "        \n",
    "        # store log_likelihood and AER values\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "        if val_sentence_pairs and reference_alignments:\n",
    "            predicted_alignments = align(lprobs, jump_probs, val_sentence_pairs)\n",
    "            AER = aer.calculate_AER(reference_alignments, predicted_alignments)\n",
    "            AERs.append(AER)\n",
    "\n",
    "        # print debug info\n",
    "        if fn_debug:\n",
    "            fn_debug(i, lprobs, log_likelihood, AER)\n",
    "\n",
    "        # update probabilities\n",
    "        for s in lprobs.keys():\n",
    "            for t in lprobs[s].keys():\n",
    "                lprobs[s][t] = counts_t_given_s[s][t]/total_s[s]\n",
    "        \n",
    "  #      print('jump_counts', [\"%.2f\"%item for item in jump_counts])\n",
    "        jump_count_sum = sum(jump_counts)\n",
    "        jump_probs = [jc/jump_count_sum for jc in jump_counts]\n",
    "    #    print(f'i:{i}, jump_probs:{[\"%.2f\"%item for item in jump_probs]}')\n",
    "\n",
    "        # update iteration number\n",
    "        i += 1\n",
    "    return lprobs, jump_probs, log_likelihoods, AERs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:54 : 0\n",
      "04:54 : 1\n",
      "04:54 : 2\n",
      "04:54 : 3\n",
      "04:54 : 4\n",
      "04:54 : 5\n",
      "04:54 : 6\n",
      "04:54 : 7\n",
      "04:54 : 8\n",
      "04:54 : 9\n",
      "PASSED!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['<NULL>', 'Haus', 'das'], ['the', 'house']),\n",
       " (['<NULL>', 'Buch', 'das'], ['the', 'book']),\n",
       " (['<NULL>', 'Buch', 'ein'], ['a', 'book']),\n",
       " (['<NULL>', 'Maus', 'ein'], ['a', 'mouse']),\n",
       " (['<NULL>', 'Hund', 'das'], ['the', 'dog']),\n",
       " (['<NULL>', 'Welt', 'ein'], ['a', 'world']),\n",
       " (['<NULL>', 'Boot', 'das'], ['the', 'boat']),\n",
       " (['<NULL>', 'Brugge', 'ein'], ['a', 'bridge'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NULL> the 0.49986849923914406\n",
      "<NULL> house 7.356813242249742e-07\n",
      "<NULL> book 0.0002585874337666559\n",
      "<NULL> a 0.49986849923914406\n",
      "<NULL> mouse 7.356813242249742e-07\n",
      "<NULL> dog 7.356813242249742e-07\n",
      "<NULL> world 7.356813242249742e-07\n",
      "<NULL> boat 7.356813242249742e-07\n",
      "<NULL> bridge 7.356813242249742e-07\n",
      "Haus the 4.7469538197000456e-07\n",
      "Haus house 0.999999525304618\n",
      "das the 0.999999998669036\n",
      "das house 3.9810413054863663e-10\n",
      "das book 1.3665150273143486e-10\n",
      "das dog 3.9810413054863663e-10\n",
      "das boat 3.9810413054863663e-10\n",
      "Buch the 9.415144932164758e-09\n",
      "Buch book 0.9999999811697101\n",
      "Buch a 9.415144932164756e-09\n",
      "ein a 0.9999999986690361\n",
      "ein book 1.3665150273143486e-10\n",
      "ein mouse 3.9810413054863663e-10\n",
      "ein world 3.9810413054863663e-10\n",
      "ein bridge 3.9810413054863663e-10\n",
      "Maus a 4.746953819700045e-07\n",
      "Maus mouse 0.999999525304618\n",
      "Hund the 4.7469538197000456e-07\n",
      "Hund dog 0.999999525304618\n",
      "Welt a 4.746953819700045e-07\n",
      "Welt world 0.999999525304618\n",
      "Boot the 4.7469538197000456e-07\n",
      "Boot boat 0.999999525304618\n",
      "Brugge a 4.746953819700045e-07\n",
      "Brugge bridge 0.999999525304618\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49999496375577407,\n",
       " 1.8100451027309263e-07,\n",
       " 0.4808581819113756,\n",
       " 0.019146673328340063]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_t_pairs, s_vocabulary, t_vocabulary = datasets.example_data_word_order()\n",
    "(lprobs, jump_probs, _, _) = EM(s_t_pairs, s_vocabulary, t_vocabulary, \n",
    "                 fn_debug = None) #debug_helpers.print_likelihood debug_helpers.print_lexicon_probs\n",
    "\n",
    "assert(round(lprobs['Haus']['house'], 5) == 1.)\n",
    "assert(round(lprobs['Welt']['world'], 5) == 1.)\n",
    "assert(round(lprobs['das']['the'], 5) == 1.)\n",
    "assert(round(lprobs['<NULL>']['house'], 5) == 0.)\n",
    "assert(round(lprobs['<NULL>']['the'], 5) == 0.49987)\n",
    "\n",
    "assert(round(jump_probs[0], 5) == 0.49999)\n",
    "assert(round(jump_probs[1], 5) == 0.0)\n",
    "assert(round(jump_probs[2], 5) == 0.48086)\n",
    "assert(round(jump_probs[3], 5) == 0.01915)\n",
    "\n",
    "print('PASSED!')\n",
    "\n",
    "s_t_pairs\n",
    "debug_helpers.print_lexicon_probs(None, lprobs, None, None)\n",
    "jump_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run EM on toy example\n",
    "s_t_pairs, s_vocabulary, t_vocabulary = datasets.example_data_null_words()\n",
    "s_t_pairs[0:3]\n",
    "(lprobs, jump_probs, _, _) = EM(s_t_pairs[0:3], s_vocabulary, t_vocabulary, \n",
    "                 fn_debug = debug_helpers.print_likelihood)\n",
    "\n",
    "debug_helpers.print_lexicon_probs(None, lprobs, None, None)\n",
    "jump_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run EM on toy example with NULL words\n",
    "val_sentence_pairs = [(\n",
    "    ['<NULL>', 'Buch', 'klein', 'das', 'Haus'], \n",
    "    ['book', 'small', 'the', 'house']\n",
    ")]\n",
    "ref_alignments = [[\n",
    "    {(3, 1), (2, 2), (4, 3), (1, 4)}, \n",
    "    {(3, 1), (2, 2), (4, 3), (1, 4)}\n",
    "]]\n",
    "s_t_pairs, s_vocabulary, t_vocabulary = datasets.example_data_null_words()\n",
    "s_t_pairs\n",
    "(lprobs, jump_probs, _, _) = EM(\n",
    "    s_t_pairs, s_vocabulary, t_vocabulary, 20,\n",
    "    val_sentence_pairs, ref_alignments, debug_helpers.print_likelihood)\n",
    "debug_helpers.print_lexicon_probs(None, lprobs, None, None)\n",
    "jump_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run EM on training data set with AER on validation set\n",
    "s_t_pairs, s_vocabulary, t_vocabulary = datasets.training_data()\n",
    "val_sentence_pairs = datasets.validation_data(s_vocabulary, t_vocabulary)\n",
    "reference_alignments = datasets.validation_alignments()    \n",
    "\n",
    "(lprobs, jump_probs, log_lhoods, AERs) = EM(\n",
    "    s_t_pairs, s_vocabulary, t_vocabulary, 30,\n",
    "    val_sentence_pairs, reference_alignments, \n",
    "    debug_helpers.print_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [9,1,2,3,4,15,4,3,2]\n",
    "(index_max, _) = max(enumerate(mylist), key=lambda t: t[1])\n",
    "index_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
