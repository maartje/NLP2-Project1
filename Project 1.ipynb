{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import datasets\n",
    "import IBM1 as ibm1\n",
    "#import aer\n",
    "\n",
    "# pretty print variabeles on line\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to output useful information after each \n",
    "# EM iteration\n",
    "\n",
    "def print_likelihood(i, lprobs, log_likelihood, aer):\n",
    "    likelihood = math.exp(log_likelihood)\n",
    "    if i == 1:\n",
    "        print('iteration  log_likelihood  likelihood  AER')\n",
    "    print(f'{i} {log_likelihood:.3f} {likelihood:.3f} {aer:.5f}')\n",
    "\n",
    "def print_lexicon_probs(i, lprobs, log_likelihood, aer):\n",
    "    for s in lprobs.keys():\n",
    "        for t in lprobs[s].keys():\n",
    "            if lprobs[s][t] > 0:\n",
    "                print (s, t, lprobs[s][t])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EM on toy example\n",
    "s_t_pairs, s_vocabulary, t_vocabulary = datasets.example_data()\n",
    "s_t_pairs\n",
    "(lprobs, _, _) = ibm1.EM(s_t_pairs, s_vocabulary, t_vocabulary, \n",
    "                 fn_after_iter = print_likelihood)\n",
    "\n",
    "print_lexicon_probs(None, lprobs, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['<NULL>', 'das', 'Haus'], ['the', 'house']),\n",
       " (['<NULL>', 'das', 'Buch'], ['the', 'book']),\n",
       " (['<NULL>', 'ein', 'Buch'], ['a', 'book']),\n",
       " (['<NULL>', 'ein', 'Haus'], ['a', 'small', 'house']),\n",
       " (['<NULL>', 'mein', 'Buch'], ['my', 'small', 'book'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  log_likelihood  likelihood  AER\n",
      "1 -8.318 0.000 0.25000\n",
      "2 -3.214 0.040 0.14286\n",
      "3 -2.152 0.116 0.14286\n",
      "4 -1.373 0.253 0.14286\n",
      "5 -0.865 0.421 0.14286\n",
      "6 -0.544 0.580 0.14286\n",
      "7 -0.338 0.713 0.14286\n",
      "8 -0.202 0.817 0.14286\n",
      "9 -0.109 0.896 0.14286\n",
      "10 -0.045 0.956 0.14286\n",
      "<NULL> the 0.02257393014477732\n",
      "<NULL> house 0.03853211664967519\n",
      "<NULL> book 0.25879136482282894\n",
      "<NULL> a 0.04036246446938904\n",
      "<NULL> small 0.6388299741911422\n",
      "<NULL> my 0.0009101497221873396\n",
      "das the 0.9982745611336503\n",
      "das house 0.001521101649152167\n",
      "das book 0.00020433721719753526\n",
      "Haus the 0.0005024136009538736\n",
      "Haus house 0.9648451634494088\n",
      "Haus a 0.0008770198645035952\n",
      "Haus small 0.03377540308513371\n",
      "Buch the 9.294634178534926e-05\n",
      "Buch book 0.9952068965931067\n",
      "Buch a 0.00017007146758518473\n",
      "Buch my 0.0035000676358473257\n",
      "Buch small 0.0010300179616753772\n",
      "ein a 0.9665889462253223\n",
      "ein book 0.00012498978556698626\n",
      "ein small 0.03230203858017701\n",
      "ein house 0.0009840254089337642\n",
      "mein my 0.7703666901172609\n",
      "mein small 0.22670748409840172\n",
      "mein book 0.0029258257843373437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run EM on toy example with NULL words\n",
    "val_sentence_pairs = [(\n",
    "    ['<NULL>', 'Buch', 'klein', 'das', 'Haus'], \n",
    "    ['the', 'small', 'house', 'book']\n",
    ")]\n",
    "ref_alignments = [[\n",
    "    {(1, 3), (2, 2), (3, 4), (4, 1)}, \n",
    "    {(1, 3), (2, 2), (3, 4), (4, 1)}\n",
    "]]\n",
    "s_t_pairs, s_vocabulary, t_vocabulary = datasets.example_data_null_words()\n",
    "s_t_pairs\n",
    "(lprobs, _, _) = ibm1.EM(s_t_pairs, s_vocabulary, t_vocabulary, \n",
    "                 val_sentence_pairs, ref_alignments, print_likelihood)\n",
    "print_lexicon_probs(None, lprobs, None, None)\n",
    "\n",
    "# QUESTION: how can likelihood be bigger than 1?\n",
    "\n",
    "# example alignment\n",
    "# alignment positions start at 1, NULL word is ignored \n",
    "#sentence_pair\n",
    "#alignment = ibm1.align(lprobs, sentence_pair)\n",
    "#alignment\n",
    "#{(1, 3), (3, 4), (4, 1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EM on training data set (no AER)\n",
    "\n",
    "s_t_pairs, s_vocabulary, t_vocabulary = datasets.training_data()\n",
    "(lprobs, log_lhoods, _) = ibm1.EM(s_t_pairs, s_vocabulary, \n",
    "        t_vocabulary, fn_after_iter = print_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  log_likelihood  likelihood  AER\n",
      "1 -34858025.333 0.000 0.79829\n",
      "2 -8405523.373 0.000 0.79195\n",
      "3 -5424561.055 0.000 0.79038\n",
      "4 -4514727.763 0.000 0.78613\n",
      "5 -4203772.133 0.000 0.78998\n",
      "6 -4064695.086 0.000 0.79191\n",
      "7 -3991189.402 0.000 0.78861\n",
      "8 -3947933.360 0.000 0.78785\n",
      "9 -3920480.549 0.000 0.79094\n",
      "10 -3902021.286 0.000 0.79094\n"
     ]
    }
   ],
   "source": [
    "# Run EM on training data set\n",
    "import aer\n",
    "\n",
    "val_naacl_path = 'validation/dev.wa.nonullalign'\n",
    "val_sentence_pairs, _, _ = datasets.validation_data()\n",
    "reference_alignments = aer.read_naacl_alignments(val_naacl_path)    \n",
    "\n",
    "s_t_pairs, s_vocabulary, t_vocabulary = datasets.training_data()\n",
    "(lprobs, log_lhoods, AERs) = ibm1.EM(s_t_pairs, s_vocabulary, t_vocabulary, val_sentence_pairs,\n",
    "                 reference_alignments, print_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NULL> .\n",
      "industrialized industrialisés\n",
      "billion milliards\n",
      "informed informé\n",
      "responsible responsable\n",
      "Honourable honorable\n",
      "bravery bravoure\n",
      "Acadian acadienne\n",
      "hometown ville\n",
      "places endroits\n",
      "reorganizing ait\n",
      "scepticism scepticisme\n",
      "writing écrit\n",
      "broader plus\n",
      "outcome résultat\n",
      "declining baisse\n",
      "reminds rappelle\n",
      "Mouton à\n",
      "deliberate délibérée\n",
      "218 218\n",
      "math calcul\n",
      "hide cacher\n",
      "solvents solvants\n",
      "severely gravement\n",
      "rusting attaqué\n",
      "nefarious savais\n",
      "abdication abdication\n",
      "hepatitis hépatite\n",
      "Cross Croix\n",
      "scapegoat bouc\n",
      "minority minorité\n",
      "orientations orientations\n",
      "televisions allumaient\n",
      "offensive choquant\n",
      "Page Page\n",
      "diesel diesel\n",
      "simplified simplifier\n",
      "dash clignotant\n",
      "readjustment un\n",
      "1947 1947\n",
      "Truro Truro\n",
      "Blais Blais\n",
      "LAKES GRANDS\n",
      "tracks inspire\n",
      "5,500 5\n",
      "wasted gaspillé\n",
      "Jimmy Jimmy\n",
      "Zarya module\n",
      "minions laquais\n",
      "galling exaspérant\n",
      "infusion injection\n",
      "Leslie Leslie\n",
      "zealous exagérée\n",
      "Thai thaïlandais\n",
      "Regulation acquisition\n",
      "1.65 1,65\n",
      "ploughing engloutir\n",
      "resemble même\n",
      "signifying qui\n",
      "Zaltzman Zaltzman\n",
      "Job pour\n",
      "Vastel Vastel\n",
      "transform transformer\n",
      "erect ériger\n",
      "tentacles tentacules\n",
      "NSERC souligner\n",
      "Gaiters coach\n",
      "Woodstock Woodstock\n",
      "secondment détachement\n",
      "podium podium\n",
      "choreography chorégraphies\n",
      "enclosed trouverez\n",
      "Alpin mondial\n",
      "submissive contre\n",
      "Coyote Coyote\n",
      "arouses pareil\n",
      "Youppi questionnerM.\n",
      "Sharing collaboreraient\n",
      "installations sujet\n",
      "Hilarion Hilarion\n",
      "spills déversements\n",
      "10.1 verser\n",
      "impart établiraient\n",
      "Hickes Hickes\n",
      "tailgunner mitrailleur\n",
      "swings fluctuations\n",
      "scruples scrupules\n",
      "Adjourned Ajournement\n",
      "Directives Personal\n",
      "shoulds gardant\n",
      "frolics fantaisies\n",
      "Bolivians que\n",
      "1102 égard\n",
      "paramour ils\n",
      "Teleglobe plus\n",
      "Railways signature\n",
      "Henner Eike\n",
      "463,000 de141\n",
      "KLA Armée\n",
      "ike teneur\n",
      "softly décadent\n"
     ]
    }
   ],
   "source": [
    "def print_example_alignments(lprobs, n = 100):\n",
    "    split = math.floor(len(lprobs.keys())/n)\n",
    "    i = 0\n",
    "    for t_word in lprobs.keys():\n",
    "        if i%split == 0:\n",
    "            max_sword = max(lprobs[t_word], key=lprobs[t_word].get)\n",
    "            print (t_word, max_sword)\n",
    "        i += 1\n",
    "print_example_alignments(lprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_pairs = val_sentence_pairs[5:15]\n",
    "ef_reference_alignments = reference_alignments[5:15]\n",
    "ef_predicted_alignments = ibm1.align(lprobs, ef_pairs)\n",
    "AER = aer.calculate_AER(ef_reference_alignments, ef_predicted_alignments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8161764705882353"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<NULL>', 'I', 'never', 'met', 'a', 'street', 'hooker', 'who', 'wanted', 'to', 'be', 'there', '.']\n",
      "['je', 'ne', 'ai', 'jamais', 'rencontré', 'une', 'seule', 'prostituée', 'de', 'rue', 'qui', 'voulait', 'exercer', 'un', 'tel', 'métier', '.']\n"
     ]
    }
   ],
   "source": [
    "print(ef_pairs[0][0])\n",
    "print(ef_pairs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1),\n",
       " (2, 2),\n",
       " (3, 1),\n",
       " (4, 2),\n",
       " (5, 3),\n",
       " (6, 4),\n",
       " (7, 3),\n",
       " (8, 1),\n",
       " (9, 9),\n",
       " (10, 5),\n",
       " (11, 7),\n",
       " (12, 8),\n",
       " (13, 5),\n",
       " (14, 4),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 12)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ef_predicted_alignments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{(1, 1), (2, 2), (2, 4), (4, 6), (5, 10), (6, 8), (7, 11), (8, 12), (12, 17)},\n",
       " {(1, 1),\n",
       "  (2, 2),\n",
       "  (2, 4),\n",
       "  (3, 3),\n",
       "  (3, 5),\n",
       "  (4, 6),\n",
       "  (5, 9),\n",
       "  (5, 10),\n",
       "  (6, 8),\n",
       "  (7, 11),\n",
       "  (8, 12),\n",
       "  (9, 13),\n",
       "  (9, 14),\n",
       "  (9, 15),\n",
       "  (9, 16),\n",
       "  (10, 13),\n",
       "  (10, 14),\n",
       "  (10, 15),\n",
       "  (10, 16),\n",
       "  (11, 13),\n",
       "  (11, 14),\n",
       "  (11, 15),\n",
       "  (11, 16),\n",
       "  (12, 17)}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ef_reference_alignments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fname = 'probs_ibm1_iter10.txt'\n",
    "json.dump(lprobs, open(fname,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_ibm1_iter_10 = json.load(open(fname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
